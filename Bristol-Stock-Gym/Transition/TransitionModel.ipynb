{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from RNN import MDNRNN\n",
    "import torch\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#  DATALOADING #\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliding_windows(data,seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[(i+1):(i+1)+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.41547799  0.00394334  2.49205041 -2.55889511  0.31533423  1.64574075\n",
      "  -0.11483234 -2.42293024  0.298       0.452       0.015       0.395\n",
      "   0.293       0.403       0.311       0.328      -0.298       1.        ]]\n",
      "[0.95917525 0.40288215 0.97879683 0.27208062 0.40316703 0.67172941\n",
      " 0.49172997 0.08352195 0.34651163 0.5255814  0.01744186 0.45930233\n",
      " 0.34069767 0.46860465 0.36162791 0.38139535 0.27456258 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "bsz = 200\n",
    "\n",
    "data = np.load('latent-action.npy')\n",
    "print(data[:,23233,:])\n",
    "#Reshaping data and seperating training and test set\n",
    "sc = MinMaxScaler()\n",
    "data = sc.fit_transform(data.squeeze(axis = 0))\n",
    "print(data[23233,:])\n",
    "seq_length = 4\n",
    "x, y = sliding_windows(data, seq_length)\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(y) * 0.67)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])).float())\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])).float())\n",
    "\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])).float())\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])).float())\n",
    "\n",
    "\n",
    "trainingset =[]\n",
    "for i in range(len(trainY)):\n",
    "    trainingset.append((trainX[i,:,:], trainY[i]))\n",
    "    \n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainingset, batch_size=bsz, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-7\n",
    "input_size = 18\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "num_classes = 1\n",
    "gaussians = 5\n",
    "\n",
    "mdnrnn = MDNRNN(z_size = input_size, n_hidden = hidden_size, n_gaussians = gaussians, n_layers = num_layers).to(device)\n",
    "\n",
    "\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500], Loss: 1.0646\n",
      "Epoch [2/500], Loss: 1.0472\n",
      "Epoch [4/500], Loss: 1.0258\n",
      "Epoch [6/500], Loss: 0.9953\n",
      "Epoch [8/500], Loss: 0.9451\n",
      "Epoch [10/500], Loss: 0.8397\n",
      "Epoch [12/500], Loss: 0.5275\n",
      "Epoch [14/500], Loss: 0.2591\n",
      "Epoch [16/500], Loss: 0.0644\n",
      "Epoch [18/500], Loss: -0.1056\n",
      "Epoch [20/500], Loss: -0.2296\n",
      "Epoch [22/500], Loss: -0.3104\n",
      "Epoch [24/500], Loss: -0.3616\n",
      "Epoch [26/500], Loss: -0.3968\n",
      "Epoch [28/500], Loss: -0.4245\n",
      "Epoch [30/500], Loss: -0.4492\n",
      "Epoch [32/500], Loss: -0.4731\n",
      "Epoch [34/500], Loss: -0.4968\n",
      "Epoch [36/500], Loss: -0.5202\n",
      "Epoch [38/500], Loss: -0.5430\n",
      "Epoch [40/500], Loss: -0.5650\n",
      "Epoch [42/500], Loss: -0.5861\n",
      "Epoch [44/500], Loss: -0.6062\n",
      "Epoch [46/500], Loss: -0.6254\n",
      "Epoch [48/500], Loss: -0.6436\n",
      "Epoch [50/500], Loss: -0.6609\n",
      "Epoch [52/500], Loss: -0.6773\n",
      "Epoch [54/500], Loss: -0.6930\n",
      "Epoch [56/500], Loss: -0.7079\n",
      "Epoch [58/500], Loss: -0.7222\n",
      "Epoch [60/500], Loss: -0.7359\n",
      "Epoch [62/500], Loss: -0.7490\n",
      "Epoch [64/500], Loss: -0.7616\n",
      "Epoch [66/500], Loss: -0.7735\n",
      "Epoch [68/500], Loss: -0.7849\n",
      "Epoch [70/500], Loss: -0.7955\n",
      "Epoch [72/500], Loss: -0.8054\n",
      "Epoch [74/500], Loss: -0.8147\n",
      "Epoch [76/500], Loss: -0.8233\n",
      "Epoch [78/500], Loss: -0.8313\n",
      "Epoch [80/500], Loss: -0.8387\n",
      "Epoch [82/500], Loss: -0.8456\n",
      "Epoch [84/500], Loss: -0.8520\n",
      "Epoch [86/500], Loss: -0.8581\n",
      "Epoch [88/500], Loss: -0.8637\n",
      "Epoch [90/500], Loss: -0.8691\n",
      "Epoch [92/500], Loss: -0.8742\n",
      "Epoch [94/500], Loss: -0.8790\n",
      "Epoch [96/500], Loss: -0.8837\n",
      "Epoch [98/500], Loss: -0.8882\n",
      "Epoch [100/500], Loss: -0.8926\n",
      "Epoch [102/500], Loss: -0.8967\n",
      "Epoch [104/500], Loss: -0.9007\n",
      "Epoch [106/500], Loss: -0.9045\n",
      "Epoch [108/500], Loss: -0.9082\n",
      "Epoch [110/500], Loss: -0.9116\n",
      "Epoch [112/500], Loss: -0.9149\n",
      "Epoch [114/500], Loss: -0.9180\n",
      "Epoch [116/500], Loss: -0.9210\n",
      "Epoch [118/500], Loss: -0.9238\n",
      "Epoch [120/500], Loss: -0.9266\n",
      "Epoch [122/500], Loss: -0.9292\n",
      "Epoch [124/500], Loss: -0.9317\n",
      "Epoch [126/500], Loss: -0.9341\n",
      "Epoch [128/500], Loss: -0.9364\n",
      "Epoch [130/500], Loss: -0.9386\n",
      "Epoch [132/500], Loss: -0.9407\n",
      "Epoch [134/500], Loss: -0.9427\n",
      "Epoch [136/500], Loss: -0.9446\n",
      "Epoch [138/500], Loss: -0.9464\n",
      "Epoch [140/500], Loss: -0.9482\n",
      "Epoch [142/500], Loss: -0.9499\n",
      "Epoch [144/500], Loss: -0.9515\n",
      "Epoch [146/500], Loss: -0.9532\n",
      "Epoch [148/500], Loss: -0.9547\n",
      "Epoch [150/500], Loss: -0.9563\n",
      "Epoch [152/500], Loss: -0.9577\n",
      "Epoch [154/500], Loss: -0.9592\n",
      "Epoch [156/500], Loss: -0.9606\n",
      "Epoch [158/500], Loss: -0.9620\n",
      "Epoch [160/500], Loss: -0.9634\n",
      "Epoch [162/500], Loss: -0.9647\n",
      "Epoch [164/500], Loss: -0.9659\n",
      "Epoch [166/500], Loss: -0.9672\n",
      "Epoch [168/500], Loss: -0.9684\n",
      "Epoch [170/500], Loss: -0.9696\n",
      "Epoch [172/500], Loss: -0.9707\n",
      "Epoch [174/500], Loss: -0.9718\n",
      "Epoch [176/500], Loss: -0.9729\n",
      "Epoch [178/500], Loss: -0.9740\n",
      "Epoch [180/500], Loss: -0.9750\n",
      "Epoch [182/500], Loss: -0.9760\n",
      "Epoch [184/500], Loss: -0.9770\n",
      "Epoch [186/500], Loss: -0.9780\n",
      "Epoch [188/500], Loss: -0.9789\n",
      "Epoch [190/500], Loss: -0.9798\n",
      "Epoch [192/500], Loss: -0.9808\n",
      "Epoch [194/500], Loss: -0.9817\n",
      "Epoch [196/500], Loss: -0.9826\n",
      "Epoch [198/500], Loss: -0.9835\n",
      "Epoch [200/500], Loss: -0.9844\n",
      "Epoch [202/500], Loss: -0.9853\n",
      "Epoch [204/500], Loss: -0.9862\n",
      "Epoch [206/500], Loss: -0.9870\n",
      "Epoch [208/500], Loss: -0.9879\n",
      "Epoch [210/500], Loss: -0.9888\n",
      "Epoch [212/500], Loss: -0.9897\n",
      "Epoch [214/500], Loss: -0.9906\n",
      "Epoch [216/500], Loss: -0.9915\n",
      "Epoch [218/500], Loss: -0.9923\n",
      "Epoch [220/500], Loss: -0.9932\n",
      "Epoch [222/500], Loss: -0.9940\n",
      "Epoch [224/500], Loss: -0.9948\n",
      "Epoch [226/500], Loss: -0.9956\n",
      "Epoch [228/500], Loss: -0.9964\n",
      "Epoch [230/500], Loss: -0.9972\n",
      "Epoch [232/500], Loss: -0.9980\n",
      "Epoch [234/500], Loss: -0.9988\n",
      "Epoch [236/500], Loss: -0.9996\n",
      "Epoch [238/500], Loss: -1.0004\n",
      "Epoch [240/500], Loss: -1.0011\n",
      "Epoch [242/500], Loss: -1.0019\n",
      "Epoch [244/500], Loss: -1.0026\n",
      "Epoch [246/500], Loss: -1.0034\n",
      "Epoch [248/500], Loss: -1.0041\n",
      "Epoch [250/500], Loss: -1.0048\n",
      "Epoch [252/500], Loss: -1.0054\n",
      "Epoch [254/500], Loss: -1.0061\n",
      "Epoch [256/500], Loss: -1.0067\n",
      "Epoch [258/500], Loss: -1.0074\n",
      "Epoch [260/500], Loss: -1.0080\n",
      "Epoch [262/500], Loss: -1.0086\n",
      "Epoch [264/500], Loss: -1.0092\n",
      "Epoch [266/500], Loss: -1.0098\n",
      "Epoch [268/500], Loss: -1.0104\n",
      "Epoch [270/500], Loss: -1.0110\n",
      "Epoch [272/500], Loss: -1.0116\n",
      "Epoch [274/500], Loss: -1.0122\n",
      "Epoch [276/500], Loss: -1.0128\n",
      "Epoch [278/500], Loss: -1.0135\n",
      "Epoch [280/500], Loss: -1.0141\n",
      "Epoch [282/500], Loss: -1.0147\n",
      "Epoch [284/500], Loss: -1.0153\n",
      "Epoch [286/500], Loss: -1.0159\n",
      "Epoch [288/500], Loss: -1.0165\n",
      "Epoch [290/500], Loss: -1.0170\n",
      "Epoch [292/500], Loss: -1.0176\n",
      "Epoch [294/500], Loss: -1.0182\n",
      "Epoch [296/500], Loss: -1.0187\n",
      "Epoch [298/500], Loss: -1.0192\n",
      "Epoch [300/500], Loss: -1.0197\n",
      "Epoch [302/500], Loss: -1.0203\n",
      "Epoch [304/500], Loss: -1.0208\n",
      "Epoch [306/500], Loss: -1.0213\n",
      "Epoch [308/500], Loss: -1.0218\n",
      "Epoch [310/500], Loss: -1.0224\n",
      "Epoch [312/500], Loss: -1.0229\n",
      "Epoch [314/500], Loss: -1.0234\n",
      "Epoch [316/500], Loss: -1.0239\n",
      "Epoch [318/500], Loss: -1.0244\n",
      "Epoch [320/500], Loss: -1.0249\n",
      "Epoch [322/500], Loss: -1.0254\n",
      "Epoch [324/500], Loss: -1.0258\n",
      "Epoch [326/500], Loss: -1.0263\n",
      "Epoch [328/500], Loss: -1.0268\n",
      "Epoch [330/500], Loss: -1.0273\n",
      "Epoch [332/500], Loss: -1.0278\n",
      "Epoch [334/500], Loss: -1.0282\n",
      "Epoch [336/500], Loss: -1.0287\n",
      "Epoch [338/500], Loss: -1.0292\n",
      "Epoch [340/500], Loss: -1.0296\n",
      "Epoch [342/500], Loss: -1.0301\n",
      "Epoch [344/500], Loss: -1.0305\n",
      "Epoch [346/500], Loss: -1.0310\n",
      "Epoch [348/500], Loss: -1.0314\n",
      "Epoch [350/500], Loss: -1.0319\n",
      "Epoch [352/500], Loss: -1.0323\n",
      "Epoch [354/500], Loss: -1.0328\n",
      "Epoch [356/500], Loss: -1.0332\n",
      "Epoch [358/500], Loss: -1.0337\n",
      "Epoch [360/500], Loss: -1.0341\n",
      "Epoch [362/500], Loss: -1.0346\n",
      "Epoch [364/500], Loss: -1.0350\n",
      "Epoch [366/500], Loss: -1.0354\n",
      "Epoch [368/500], Loss: -1.0358\n",
      "Epoch [370/500], Loss: -1.0362\n",
      "Epoch [372/500], Loss: -1.0367\n",
      "Epoch [374/500], Loss: -1.0371\n",
      "Epoch [376/500], Loss: -1.0375\n",
      "Epoch [378/500], Loss: -1.0379\n",
      "Epoch [380/500], Loss: -1.0383\n",
      "Epoch [382/500], Loss: -1.0387\n",
      "Epoch [384/500], Loss: -1.0391\n",
      "Epoch [386/500], Loss: -1.0395\n",
      "Epoch [388/500], Loss: -1.0398\n",
      "Epoch [390/500], Loss: -1.0402\n",
      "Epoch [392/500], Loss: -1.0406\n",
      "Epoch [394/500], Loss: -1.0410\n",
      "Epoch [396/500], Loss: -1.0413\n",
      "Epoch [398/500], Loss: -1.0417\n",
      "Epoch [400/500], Loss: -1.0421\n",
      "Epoch [402/500], Loss: -1.0424\n",
      "Epoch [404/500], Loss: -1.0428\n",
      "Epoch [406/500], Loss: -1.0431\n",
      "Epoch [408/500], Loss: -1.0435\n",
      "Epoch [410/500], Loss: -1.0438\n",
      "Epoch [412/500], Loss: -1.0441\n",
      "Epoch [414/500], Loss: -1.0445\n",
      "Epoch [416/500], Loss: -1.0448\n",
      "Epoch [418/500], Loss: -1.0451\n",
      "Epoch [420/500], Loss: -1.0454\n",
      "Epoch [422/500], Loss: -1.0458\n",
      "Epoch [424/500], Loss: -1.0460\n",
      "Epoch [426/500], Loss: -1.0463\n",
      "Epoch [428/500], Loss: -1.0466\n",
      "Epoch [430/500], Loss: -1.0468\n",
      "Epoch [432/500], Loss: -1.0471\n",
      "Epoch [434/500], Loss: -1.0473\n",
      "Epoch [436/500], Loss: -1.0475\n",
      "Epoch [438/500], Loss: -1.0477\n",
      "Epoch [440/500], Loss: -1.0479\n",
      "Epoch [442/500], Loss: -1.0482\n",
      "Epoch [444/500], Loss: -1.0484\n",
      "Epoch [446/500], Loss: -1.0486\n",
      "Epoch [448/500], Loss: -1.0488\n",
      "Epoch [450/500], Loss: -1.0490\n",
      "Epoch [452/500], Loss: -1.0492\n",
      "Epoch [454/500], Loss: -1.0495\n",
      "Epoch [456/500], Loss: -1.0497\n",
      "Epoch [458/500], Loss: -1.0499\n",
      "Epoch [460/500], Loss: -1.0501\n",
      "Epoch [462/500], Loss: -1.0504\n",
      "Epoch [464/500], Loss: -1.0506\n",
      "Epoch [466/500], Loss: -1.0508\n",
      "Epoch [468/500], Loss: -1.0510\n",
      "Epoch [470/500], Loss: -1.0513\n",
      "Epoch [472/500], Loss: -1.0515\n",
      "Epoch [474/500], Loss: -1.0517\n",
      "Epoch [476/500], Loss: -1.0519\n",
      "Epoch [478/500], Loss: -1.0521\n",
      "Epoch [480/500], Loss: -1.0522\n",
      "Epoch [482/500], Loss: -1.0524\n",
      "Epoch [484/500], Loss: -1.0526\n",
      "Epoch [486/500], Loss: -1.0527\n",
      "Epoch [488/500], Loss: -1.0529\n",
      "Epoch [490/500], Loss: -1.0531\n",
      "Epoch [492/500], Loss: -1.0532\n",
      "Epoch [494/500], Loss: -1.0534\n",
      "Epoch [496/500], Loss: -1.0536\n",
      "Epoch [498/500], Loss: -1.0538\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(mdnrnn.parameters(), lr = learning_rate)\n",
    "\n",
    "epochs = 500\n",
    "random = torch.zeros(200,4,17)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    hidden = mdnrnn.init_hidden(bsz)\n",
    "\n",
    "    # Get mini-batch inputs and targets\n",
    "    total_loss = 0\n",
    "    j = 0\n",
    "    for i, (inputs, targets) in enumerate(trainloader):\n",
    "        # Forward pass\n",
    "        if inputs.size(0) != bsz:\n",
    "            continue\n",
    "        hidden = detach(hidden)\n",
    "        (pi, mu, sigma), hidden = mdnrnn(inputs.to(device), hidden)\n",
    "\n",
    "        loss = mdnrnn.criterion(targets.to(device), pi, mu, sigma)\n",
    "        # Backward and optimize\n",
    "        mdnrnn.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(mdnrnn.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        j += 1\n",
    "    total_loss /=j\n",
    "    if epoch % 2 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch, epochs, total_loss))\n",
    "        torch.save(mdnrnn.state_dict(), '../Models/transition-regression-new')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.697392  , 0.7311963 , 0.78749067, 0.34954572, 0.22417575,\n",
      "        0.81833375, 0.44740587, 0.26787177, 0.2767442 , 0.4616279 ,\n",
      "        0.29651162, 0.6662791 , 0.6465116 , 0.3604651 , 0.38139534,\n",
      "        0.6488372 , 0.31493944, 0.        ]], dtype=float32), array([[0.697392  , 0.7311963 , 0.78749067, 0.34954572, 0.22417575,\n",
      "        0.81833375, 0.44740587, 0.26787177, 0.2767442 , 0.4616279 ,\n",
      "        0.29651162, 0.6662791 , 0.6465116 , 0.3604651 , 0.38139534,\n",
      "        0.6488372 , 0.31493944, 0.        ]], dtype=float32), array([[0.697392  , 0.7311963 , 0.78749067, 0.34954572, 0.22417575,\n",
      "        0.81833375, 0.44740587, 0.26787177, 0.2767442 , 0.4616279 ,\n",
      "        0.29651162, 0.6662791 , 0.6465116 , 0.3604651 , 0.38139534,\n",
      "        0.6488372 , 0.31493944, 0.        ]], dtype=float32), array([[0.697392  , 0.7311963 , 0.78749067, 0.34954572, 0.22417575,\n",
      "        0.81833375, 0.44740587, 0.26787177, 0.2767442 , 0.4616279 ,\n",
      "        0.29651162, 0.6662791 , 0.6465116 , 0.3604651 , 0.38139534,\n",
      "        0.6488372 , 0.31493944, 0.        ]], dtype=float32), array([[0.697392  , 0.7311963 , 0.78749067, 0.34954572, 0.22417575,\n",
      "        0.81833375, 0.44740587, 0.26787177, 0.2767442 , 0.4616279 ,\n",
      "        0.29651162, 0.6662791 , 0.6465116 , 0.3604651 , 0.38139534,\n",
      "        0.6488372 , 0.31493944, 0.        ]], dtype=float32)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-bbe9e8ed25d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdata_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdataY_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0;32m--> 434\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    538\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "mdnrnn.load_state_dict(torch.load('../Models/transition-regression-new', map_location='cpu'))\n",
    "\n",
    "mdnrnn.eval()\n",
    "size = train_size+test_size\n",
    "zero = np.random.randint(testX.size(0))\n",
    "\n",
    "one = np.random.randint(testX.size(1))\n",
    "\n",
    "\n",
    "x = testX[zero:zero+1, one:one+1, :]\n",
    "y = testX[zero:zero+1, one+1:one+2, :]\n",
    "\n",
    "hidden = mdnrnn.init_hidden(1)\n",
    "(pi, mu, sigma), _ = mdnrnn(x, hidden)\n",
    "\n",
    "y_preds = [torch.normal(mu, sigma)[:, :, i, :] for i in range(5)]\n",
    "\n",
    "new = [x.detach().numpy().reshape(1,18) for tens in y_preds]\n",
    "\n",
    "print(new)\n",
    "\n",
    "data_predict = sc.inverse_transform(new[0].reshape(1,-1))\n",
    "dataY_truth = sc.inverse_transform(y.flatten().reshape(1,))\n",
    "print(data_predict[0:5])\n",
    "print(\"---------------------------------------\")\n",
    "print(data_truth[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mdnrnn.state_dict(), '../Models/transition-regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sc_midprice', 'wb') as f:\n",
    "    pickle.dump(sc_y, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564biteac121e2b5f64df2ac86de1e25af1041"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
