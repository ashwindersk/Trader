{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNN2 import LSTM\n",
    "import torch\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#  DATALOADING #\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliding_windows(data,seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[(i+seq_length):(i+1)+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.41547799  0.00394334  2.49205041 -2.55889511  0.31533423  1.64574075\n",
      "  -0.11483234 -2.42293024  0.298       0.452       0.015       0.395\n",
      "   0.293       0.403       0.311       0.328      -0.298       1.        ]]\n",
      "[0.95917525 0.40288215 0.97879683 0.27208062 0.40316703 0.67172941\n",
      " 0.49172997 0.08352195 0.34651163 0.5255814  0.01744186 0.45930233\n",
      " 0.34069767 0.46860465 0.36162791 0.38139535 0.27456258 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "bsz = 200\n",
    "\n",
    "data = np.load('latent-action.npy')\n",
    "print(data[:,23233,:])\n",
    "#Reshaping data and seperating training and test set\n",
    "sc = MinMaxScaler()\n",
    "data = sc.fit_transform(data.squeeze(axis = 0))\n",
    "print(data[23233,:])\n",
    "seq_length = 4\n",
    "x, y = sliding_windows(data, seq_length)\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(y) * 0.67)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])).float())\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])).float())\n",
    "\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])).float())\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])).float())\n",
    "\n",
    "\n",
    "trainingset =[]\n",
    "for i in range(len(trainY)):\n",
    "    trainingset.append((trainX[i,:,:], trainY[i]))\n",
    "    \n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainingset, batch_size=bsz, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-7\n",
    "input_size = 18\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "num_classes = 1\n",
    "fc1_out = 128\n",
    "output_size = 18\n",
    "\n",
    "lstm = LSTM(output_size, input_size, hidden_size, num_layers, seq_length, fc1_out)\n",
    "\n",
    "\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.train()\n",
    "test_hist = []\n",
    "criterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(500):\n",
    "\n",
    "        outputs = lstm(trainX).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # obtain the loss function\n",
    "        loss = criterion(outputs, trainY)\n",
    "    \n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        if testX is not None:\n",
    "            with torch.no_grad():\n",
    "                y_test_pred = lstm(testX).to(device)\n",
    "                test_loss = criterion(y_test_pred.float(), testY).to(device)\n",
    "                test_hist.append( test_loss.item())\n",
    "\n",
    "        print(\"Epoch: %d, train loss: %1.5f, test loss: %1.5f\" % (epoch, loss.item(), test_hist[epoch]))\n",
    "\n",
    "        torch.save(lstm.state_dict(), '../Models/test-transition')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdnrnn.load_state_dict(torch.load('../Models/transition-regression-test', map_location='cpu'))\n",
    "\n",
    "mdnrnn.eval()\n",
    "size = train_size+test_size\n",
    "zero = np.random.randint(testX.size(0))\n",
    "\n",
    "one = np.random.randint(testX.size(1))\n",
    "\n",
    "\n",
    "x = testX[zero:zero+1, one:one+1, :]\n",
    "y = testX[zero:zero+1, one+1:one+2, :]\n",
    "print(x.shape)\n",
    "hidden = mdnrnn.init_hidden(1)\n",
    "(pi, mu, sigma), _ = mdnrnn(x, hidden)\n",
    "\n",
    "y_preds = [torch.normal(mu, sigma)[:, :, i, :] for i in range(5)]\n",
    "\n",
    "new = [tens.detach().numpy().reshape(1,18) for tens in y_preds]\n",
    "\n",
    "\n",
    "print(y.shape)\n",
    "data_predict = new[2]\n",
    "dataY_truth = y.flatten()\n",
    "print(data_predict)\n",
    "print(\"---------------------------------------\")\n",
    "print(dataY_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([0.9825, 0.4345, 0.9836, 0.2939, 0.3526, 0.7257, 0.4492, 0.1332,\n",
    "          0.5326, 0.5326, 0.5326, 0.5349, 0.5337, 0.5326, 0.0012, 0.0000,\n",
    "          0.4744, 0.5000])\n",
    "x = x.reshape((1,1,18))\n",
    "(pi,mu,sigma), _ = mdnrnn(x,hidden)\n",
    "pi = pi.reshape((1,5,18))\n",
    "y_preds = [torch.normal(mu, sigma)[:, :, i, :] for i in range(5)]\n",
    "y_preds = [tens.reshape(1,18) for tens in y_preds]\n",
    "y_preds = torch.cat(y_preds, dim = 0)\n",
    "\n",
    "y_pred = pi * y_preds\n",
    "            \n",
    "\n",
    "y_pred = torch.sum(y_pred, dim = 1)\n",
    "\n",
    "            #print(y_pred)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_pred = y_pred.reshape((1,18))\n",
    "y_pred = sc.inverse_transform(y_pred)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mdnrnn.state_dict(), '../Models/transition-regression-new2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sc', 'wb') as f:\n",
    "    pickle.dump(sc, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564biteac121e2b5f64df2ac86de1e25af1041"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
