\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\providecommand \oddpage@label [2]{}
\citation{https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf}
\citation{DQN}
\citation{BSE}
\citation{BSG}
\citation{BC4}
\citation{limit-order}
\citation{algorithmic-trading-wiki}
\citation{MarketEquilibrium}
\citation{https://www.investopedia.com/terms/c/competitive-equilibriums.asp}
\citation{Gode and Sunder}
\citation{https://www.economicshelp.org/blog/glossary/allocative-efficiency/}
\citation{Cliff-critique}
\citation{GD}
\citation{IBM experiments}
\citation{AA}
\citation{FinTech Lecture Slides}
\citation{FinTech Lecture Slides}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Contextual Background}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:context}{{1}{1}{Contextual Background}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}History of Research}{1}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Experimental Economics to Algorithmic Trading}{1}{subsection.1.2.1}}
\newlabel{section:history}{{1.2.1}{1}{Experimental Economics to Algorithmic Trading}{subsection.1.2.1}{}}
\citation{Deep Learning Revolution}
\citation{RL-history}
\citation{deep learning robots https://arxiv.org/abs/1504.00702}
\citation{mx banks profit breakdown https://www.mx.com/moneysummit/top-us-retail-banks-income-revenue}
\citation{quora-trading-hard}
\citation{https://www.statista.com/chart/20245/share-of-computerized-and-human-trading-in-us-equities/}
\citation{https://www.statista.com/chart/20245/share-of-computerized-and-human-trading-in-us-equities/}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Developments in Artificial Intelligence}{3}{subsection.1.2.2}}
\newlabel{ section:DevelopmentsAI}{{1.2.2}{3}{Developments in Artificial Intelligence}{subsection.1.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Motivations}{3}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Finance perspective}{3}{subsection.1.3.1}}
\newlabel{subsection:Finance perspective}{{1.3.1}{3}{Finance perspective}{subsection.1.3.1}{}}
\citation{deepmind alpha go zero}
\citation{falcao}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Computer Science perspective}{4}{subsection.1.3.2}}
\newlabel{subsection:Computer Science perspective}{{1.3.2}{4}{Computer Science perspective}{subsection.1.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Furthering previous work}{4}{subsection.1.3.3}}
\newlabel{subsection:Furthering previous work}{{1.3.3}{4}{Furthering previous work}{subsection.1.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Central Challenges}{5}{section.1.4}}
\newlabel{section:Central Challenges}{{1.4}{5}{Central Challenges}{section.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Conclusion}{5}{section.1.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Technical Background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:technical}{{2}{7}{Technical Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Core Problem}{7}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Limit Order Book}{7}{subsection.2.1.1}}
\newlabel{subsection:The Limit Order Book}{{2.1.1}{7}{The Limit Order Book}{subsection.2.1.1}{}}
\newlabel{Technical:LOB}{{2.1.1}{7}{The Limit Order Book}{subsection.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A graphical representation of a Limit Order Book (LOB), reproduced from Cliff (2018)\relax }}{7}{figure.caption.10}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Lob}{{2.1}{7}{A graphical representation of a Limit Order Book (LOB), reproduced from Cliff (2018)\relax }{figure.caption.10}{}}
\newlabel{microprice}{{2.1}{8}{The Limit Order Book}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A graphical representation of a Limit Order Book (LOB), reproduced from Cliff (2018)\relax }}{8}{figure.caption.11}}
\newlabel{fig:LobPost}{{2.2}{8}{A graphical representation of a Limit Order Book (LOB), reproduced from Cliff (2018)\relax }{figure.caption.11}{}}
\citation{hey}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Proprietary trader's Objective}{9}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Existing Solutions}{9}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning}{9}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}What is Reinforcement Learning}{9}{subsection.2.3.1}}
\citation{rearrange-value-function https://www.jeremyjordan.me/markov-decision-process/}
\citation{rearrange-value-function https://www.jeremyjordan.me/markov-decision-process}
\newlabel{discount-rewards}{{2.3}{10}{What is Reinforcement Learning}{equation.2.3.3}{}}
\newlabel{expected-discounted-rewards}{{2.4}{10}{What is Reinforcement Learning}{equation.2.3.4}{}}
\newlabel{expected-discounted-rewards-rephrased}{{2.5}{10}{What is Reinforcement Learning}{equation.2.3.5}{}}
\newlabel{action-value}{{2.6}{10}{What is Reinforcement Learning}{equation.2.3.6}{}}
\newlabel{bellman}{{2.7}{10}{What is Reinforcement Learning}{equation.2.3.7}{}}
\citation{MDP-theorem RL Course by David Silver - Lecture 2: Markov Decision Process}
\newlabel{value-function-bellman}{{2.8}{11}{What is Reinforcement Learning}{equation.2.3.8}{}}
\newlabel{action-value-bellman}{{2.9}{11}{What is Reinforcement Learning}{equation.2.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Optimality}{11}{subsection.2.3.2}}
\newlabel{Optimality}{{2.3.2}{11}{Optimality}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimal Value Function.}{11}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{Optimal Action-Value Function.}{11}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Optimal Policy.}{11}{section*.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Challenges to Find Optimal Solutions}{11}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Solving Reinforcement Learning}{11}{section.2.4}}
\newlabel{section:Solving Reinforcment Learning}{{2.4}{11}{Solving Reinforcement Learning}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Model Free Prediction: Monte Carlo }{12}{subsection.2.4.1}}
\newlabel{Monte Carlo Prediction}{{2.4.1}{12}{Model Free Prediction: Monte Carlo}{subsection.2.4.1}{}}
\newlabel{Monte Carlo Mean}{{2.10}{12}{Model Free Prediction: Monte Carlo}{equation.2.4.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Model Free Control: Monte Carlo}{12}{subsection.2.4.2}}
\newlabel{Monte Carlo Control}{{2.11}{12}{Model Free Control: Monte Carlo}{equation.2.4.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Model Free: Temporal Difference Learning - TD(0)}{12}{subsection.2.4.3}}
\newlabel{TD learning}{{2.4.3}{12}{Model Free: Temporal Difference Learning - TD(0)}{subsection.2.4.3}{}}
\newlabel{TD error}{{2.12}{12}{Model Free: Temporal Difference Learning - TD(0)}{equation.2.4.12}{}}
\citation{https://www.cellstrat.com/2020/04/27/temporal-difference-methods/}
\newlabel{TD error}{{2.13}{13}{Model Free: Temporal Difference Learning - TD(0)}{equation.2.4.13}{}}
\newlabel{SARSA}{{2.14}{13}{Model Free: Temporal Difference Learning - TD(0)}{equation.2.4.14}{}}
\newlabel{Q-learning}{{2.15}{13}{Model Free: Temporal Difference Learning - TD(0)}{equation.2.4.15}{}}
\newlabel{SARSA}{{2.16}{13}{Model Free: Temporal Difference Learning - TD(0)}{equation.2.4.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Value Function Approximation}{13}{subsection.2.4.4}}
\newlabel{subsection:value-function-approx}{{2.4.4}{13}{Value Function Approximation}{subsection.2.4.4}{}}
\citation{https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf}
\citation{https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf}
\citation{https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf}
\newlabel{value-approx}{{2.17}{14}{Value Function Approximation}{equation.2.4.17}{}}
\newlabel{action-value-approx}{{2.18}{14}{Value Function Approximation}{equation.2.4.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Types of value approximators, reproduced from Silver \cite  {https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf}\relax }}{14}{figure.caption.15}}
\newlabel{fig:Approximators}{{2.3}{14}{Types of value approximators, reproduced from Silver \cite {https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf}\relax }{figure.caption.15}{}}
\newlabel{parameter-gradient}{{2.19}{14}{Value Function Approximation}{equation.2.4.19}{}}
\citation{score-function}
\newlabel{MSE-supervised}{{2.20}{15}{Value Function Approximation}{equation.2.4.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Policy Gradient}{15}{subsection.2.4.5}}
\newlabel{subsection:Policy Gradient}{{2.4.5}{15}{Policy Gradient}{subsection.2.4.5}{}}
\newlabel{Policy Gradient}{{2.21}{15}{Policy Gradient}{equation.2.4.21}{}}
\newlabel{Policy Gradient}{{2.22}{15}{Policy Gradient}{equation.2.4.22}{}}
\newlabel{Policy Gradient}{{2.23}{15}{Policy Gradient}{equation.2.4.23}{}}
\newlabel{Policy Gradient}{{2.24}{15}{Policy Gradient}{equation.2.4.24}{}}
\newlabel{Policy Gradient}{{2.26}{16}{Policy Gradient}{equation.2.4.26}{}}
\newlabel{Policy Gradient}{{2.27}{16}{Policy Gradient}{equation.2.4.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Discrete vs Continuous Action Spaces}{16}{subsection.2.4.6}}
\newlabel{subsection:Discrete Vs Continuous Action Spaces}{{2.4.6}{16}{Discrete vs Continuous Action Spaces}{subsection.2.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Summary}{17}{section.2.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Project Execution}{19}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:execution}{{3}{19}{Project Execution}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Origin}{19}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Deeply Reinforced Trader}{19}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Architecture of Reinforcement Learning Environments\relax }}{19}{figure.caption.16}}
\newlabel{fig:RL-arch}{{\caption@xref {fig:RL-arch}{ on input line 1007}}{19}{The Deeply Reinforced Trader}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Training Environment}{20}{subsection.3.2.1}}
\newlabel{main-loop}{{3.1}{20}{Main loop to step through trading environment}{lstlisting.3.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}Main loop to step through trading environment}{20}{lstlisting.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Designing the State}{20}{subsection.3.2.2}}
\newlabel{BSG-Obs}{{3.2}{20}{BSG Environment Observation: LOB}{lstlisting.3.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}BSG Environment Observation: LOB}{20}{lstlisting.3.2}}
\citation{Deep learning for limit order books paper in research folder}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Capturing LOB Data\relax }}{21}{figure.caption.17}}
\newlabel{fig:LOB-Snapshot}{{3.2}{21}{Capturing LOB Data\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Architecture of Auto-Encoder\relax }}{22}{figure.caption.18}}
\newlabel{fig:AE-arch}{{\caption@xref {fig:AE-arch}{ on input line 1106}}{22}{Designing the State}{figure.caption.18}{}}
\newlabel{Final state}{{3.1}{23}{Designing the State}{equation.3.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Varying Supply and Demand Schedules}{23}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Discretised Strategy}{23}{subsection.3.2.4}}
\newlabel{subsection:Discretised Strategy}{{3.2.4}{23}{Discretised Strategy}{subsection.3.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Vanilla Reward}{23}{subsection.3.2.5}}
\newlabel{subsection:Vanilla Reward}{{3.2.5}{23}{Vanilla Reward}{subsection.3.2.5}{}}
\newlabel{Vanilla Reward}{{3.2}{23}{Vanilla Reward}{equation.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}The PG Vanilla Agent}{24}{subsection.3.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.7}Analysis and Evaluation of PG Vanilla}{24}{subsection.3.2.7}}
\newlabel{subsection:Analysis and Evaluation of PG Vanilla}{{3.2.7}{24}{Analysis and Evaluation of PG Vanilla}{subsection.3.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Total reward over 100 iterations of the PG Agent in a market evenly populated with ZIC, ZIP and GVWY traders. \relax }}{24}{figure.caption.19}}
\newlabel{fig:PG-Vanilla-Reward}{{3.4}{24}{Total reward over 100 iterations of the PG Agent in a market evenly populated with ZIC, ZIP and GVWY traders. \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A plot of the number transactions the PG Vanilla agent engaged in during each episode \relax }}{25}{figure.caption.20}}
\newlabel{fig:PG-Vanilla-Num-Trades}{{3.5}{25}{A plot of the number transactions the PG Vanilla agent engaged in during each episode \relax }{figure.caption.20}{}}
\citation{https://medium.com/@BonsaiAI/deep-reinforcement-learning-models-tips-tricks-for-writing-reward-functions-a84fe525e8e0}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A plot of the number of profitable trades and trades that incur a loss for the trader \relax }}{26}{figure.caption.21}}
\newlabel{fig:PG-Vanilla-Good-Vs-Bad}{{3.6}{26}{A plot of the number of profitable trades and trades that incur a loss for the trader \relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.8}Optimising The Reward Function}{26}{subsection.3.2.8}}
\newlabel{Vanilla Reward}{{3.3}{27}{Optimising The Reward Function}{equation.3.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.9}Analysis of Optimised Reward System}{28}{subsection.3.2.9}}
\newlabel{subsection:Analysis and Evaluation of PG Vanilla}{{3.2.9}{28}{Analysis of Optimised Reward System}{subsection.3.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Total reward over 100 iterations of the PG Agent with an optimised reward system \relax }}{28}{figure.caption.22}}
\newlabel{fig:PG-Optimised-Balance}{{3.7}{28}{Total reward over 100 iterations of the PG Agent with an optimised reward system \relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces A plot of the number of profitable trades and trades that incur a loss for PG-Optimised \relax }}{28}{figure.caption.23}}
\newlabel{fig:PG-Optimised-Good-Vs-Bad}{{3.8}{28}{A plot of the number of profitable trades and trades that incur a loss for PG-Optimised \relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.10}Continuous Actions}{29}{subsection.3.2.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.11}DDPG}{29}{subsection.3.2.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.12}Optimising Reward Functions}{29}{subsection.3.2.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.13}Sparsity in rewards}{29}{subsection.3.2.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.14}Simpler state space}{29}{subsection.3.2.14}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Critical Evaluation}{31}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:evaluation}{{4}{31}{Critical Evaluation}{chapter.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{33}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{33}{Conclusion}{chapter.5}{}}
\bibdata{dissertation}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}An Example Appendix}{35}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appx:example}{{A}{35}{An Example Appendix}{appendix.A}{}}
