\BOOKMARK [0][-]{chapter.1}{Contextual Background}{}% 1
\BOOKMARK [1][-]{section.1.1}{Introduction}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{History of Research}{chapter.1}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{Experimental Economics to Algorithmic Trading}{section.1.2}% 4
\BOOKMARK [2][-]{subsection.1.2.2}{Developments in Artificial Intelligence}{section.1.2}% 5
\BOOKMARK [1][-]{section.1.3}{Motivations}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.3.1}{Finance perspective}{section.1.3}% 7
\BOOKMARK [2][-]{subsection.1.3.2}{Computer science perspective}{section.1.3}% 8
\BOOKMARK [2][-]{subsection.1.3.3}{Furthering previous work}{section.1.3}% 9
\BOOKMARK [1][-]{section.1.4}{Central Challenges}{chapter.1}% 10
\BOOKMARK [1][-]{section.1.5}{Conclusion}{chapter.1}% 11
\BOOKMARK [0][-]{chapter.2}{Technical Background}{}% 12
\BOOKMARK [1][-]{section.2.1}{The Core Problem}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.1.1}{The Limit Order Book}{section.2.1}% 14
\BOOKMARK [2][-]{subsection.2.1.2}{Proprietary trader's Objective}{section.2.1}% 15
\BOOKMARK [1][-]{section.2.2}{Traditional Sales Trader Solutions}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.3}{Reinforcement Learning}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.3.1}{What is Reinforcement Learning?}{section.2.3}% 18
\BOOKMARK [2][-]{subsection.2.3.2}{Optimality}{section.2.3}% 19
\BOOKMARK [2][-]{subsection.2.3.3}{Challenges to Find Optimal Solutions}{section.2.3}% 20
\BOOKMARK [1][-]{section.2.4}{Solving Reinforcement Learning}{chapter.2}% 21
\BOOKMARK [2][-]{subsection.2.4.1}{Model Free Prediction: Monte Carlo }{section.2.4}% 22
\BOOKMARK [2][-]{subsection.2.4.2}{Model Free Control: Monte Carlo}{section.2.4}% 23
\BOOKMARK [2][-]{subsection.2.4.3}{Model Free: Temporal Difference Learning - TD\(0\)}{section.2.4}% 24
\BOOKMARK [2][-]{subsection.2.4.4}{Value Function Approximation}{section.2.4}% 25
\BOOKMARK [2][-]{subsection.2.4.5}{Policy Gradient}{section.2.4}% 26
\BOOKMARK [2][-]{subsection.2.4.6}{Deep Deterministic Policy Gradient - \(DDPG\)}{section.2.4}% 27
\BOOKMARK [2][-]{subsection.2.4.7}{Discrete vs Continuous Action Spaces}{section.2.4}% 28
\BOOKMARK [1][-]{section.2.5}{Summary}{chapter.2}% 29
\BOOKMARK [0][-]{chapter.3}{Project Execution}{}% 30
\BOOKMARK [1][-]{section.3.1}{Origin}{chapter.3}% 31
\BOOKMARK [1][-]{section.3.2}{The Deeply Reinforced Trader}{chapter.3}% 32
\BOOKMARK [2][-]{subsection.3.2.1}{Training Environment}{section.3.2}% 33
\BOOKMARK [2][-]{subsection.3.2.2}{Designing the State}{section.3.2}% 34
\BOOKMARK [2][-]{subsection.3.2.3}{Discretised Strategy}{section.3.2}% 35
\BOOKMARK [2][-]{subsection.3.2.4}{Vanilla Reward}{section.3.2}% 36
\BOOKMARK [2][-]{subsection.3.2.5}{The PG Vanilla Agent}{section.3.2}% 37
\BOOKMARK [2][-]{subsection.3.2.6}{Analysis and Evaluation of PG Vanilla}{section.3.2}% 38
\BOOKMARK [2][-]{subsection.3.2.7}{Optimising The Reward Function}{section.3.2}% 39
\BOOKMARK [2][-]{subsection.3.2.8}{Analysis of Optimised Reward System}{section.3.2}% 40
\BOOKMARK [2][-]{subsection.3.2.9}{Continuous Actions}{section.3.2}% 41
\BOOKMARK [2][-]{subsection.3.2.10}{The DDPG Agent}{section.3.2}% 42
\BOOKMARK [2][-]{subsection.3.2.11}{Analysis and Evaluation of DDPG Agent}{section.3.2}% 43
\BOOKMARK [2][-]{subsection.3.2.12}{Sparsity in rewards}{section.3.2}% 44
\BOOKMARK [0][-]{chapter.4}{Conclusion}{}% 45
\BOOKMARK [1][-]{section.4.1}{Overview}{chapter.4}% 46
\BOOKMARK [1][-]{section.4.2}{Project Status}{chapter.4}% 47
\BOOKMARK [1][-]{section.4.3}{Future Work}{chapter.4}% 48
\BOOKMARK [2][-]{subsection.4.3.1}{Exploration of State Space Design}{section.4.3}% 49
\BOOKMARK [2][-]{subsection.4.3.2}{Regression Models}{section.4.3}% 50
\BOOKMARK [2][-]{subsection.4.3.3}{Increase explorative power}{section.4.3}% 51
\BOOKMARK [1][-]{section.4.4}{Last Words}{chapter.4}% 52
